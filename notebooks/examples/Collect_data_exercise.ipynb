{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports Libraries and Declares functions and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "import hashlib\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "import time\n",
    "from random import randint\n",
    "\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Functions\n",
    "'''\n",
    "def make_news_page_url(d, p):\n",
    "    if d == '' or d == None or p == '' or p == None:\n",
    "        return None\n",
    "    base_url = 'https://news.daum.net/breakingnews/society/affair?page={}&regDate={}'\n",
    "    url = base_url.format(p, d)\n",
    "    return url\n",
    "\n",
    "def make_news_contents_url(code):\n",
    "    if code == '' or code == None:\n",
    "        return None\n",
    "    base_url = 'https://news.v.daum.net/v/{}'\n",
    "    url = base_url.format(code)\n",
    "    return url\n",
    "\n",
    "def fetch_html(url):\n",
    "    try:\n",
    "        res = requests.get(url)\n",
    "        html = None\n",
    "        if res.status_code == 200:\n",
    "            html = res.text\n",
    "        return html\n",
    "    except Exception as e:\n",
    "        print('error message: ', e)\n",
    "        return None\n",
    "    \n",
    "def select_tags(html, selector):\n",
    "    try:\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        tags = soup.select(selector)\n",
    "        return tags\n",
    "    except Exception as e:\n",
    "        print('e: ', e)\n",
    "        return None\n",
    "    \n",
    "# 뉴스 목록 데이터 생성\n",
    "def create_news_resource_links(day):\n",
    "    page_check = True\n",
    "    page = 0\n",
    "    resource = []\n",
    "    print('day : ', day)\n",
    "    while page_check == True:\n",
    "        page += 1\n",
    "        url = make_news_page_url(day, page)\n",
    "        page_list_html = fetch_html(url)\n",
    "        if page_list_html == None:\n",
    "            page_check = False\n",
    "\n",
    "        tags = select_tags(page_list_html, '#mArticle > div.box_etc > ul > li')\n",
    "        if len(tags) == 0:\n",
    "            page_check = False\n",
    "            break\n",
    "\n",
    "        # UTC\n",
    "        datetime = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time()))\n",
    "        element = {\n",
    "            'resource_id': None,\n",
    "            'news_page_info': str(day) + '-' + str(page),\n",
    "            'counts': len(tags),\n",
    "            'news_ids': None,\n",
    "            'created_at': datetime,\n",
    "            'updated_at': datetime\n",
    "        }\n",
    "        codes = []\n",
    "        for index, tag in enumerate(tags):\n",
    "            anchor = tag.select_one('a')\n",
    "            code = anchor['href'].replace('https://v.daum.net/v/', '')\n",
    "            codes.append(code)\n",
    "            element['news_ids'] = codes\n",
    "\n",
    "        z = pickle.dumps(element)\n",
    "        id = hashlib.md5(z).hexdigest()\n",
    "        element['resource_id'] = id\n",
    "        resource.append(element)\n",
    "\n",
    "        # Interval\n",
    "        n = randint(2, 5)\n",
    "        time.sleep(n)\n",
    "        log = 'Page No.{} ( {}sec )'.format(page, n)\n",
    "        print(log)\n",
    "    return resource\n",
    "    \n",
    "def create_news_contents_data(html, code):\n",
    "    # status: 0 => Fail, 1 => Success\n",
    "    url = 'https://v.daum.net/v/{}'.format(code)\n",
    "    try:\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        title = soup.select_one('#cSub > div > h3').text\n",
    "        reg_date = soup.select_one('#cSub > div > span > span:nth-of-type(2)').text.replace('입력 ', '')\n",
    "        t = reg_date.split(' ')\n",
    "        d = t[0].replace('.', '-')[:-1] + ' ' + t[1]\n",
    "        contents = soup.select('#harmonyContainer > section > p')\n",
    "        sentences = []\n",
    "        for p in contents:\n",
    "            sentences.append(p.text)\n",
    "\n",
    "        news = {\n",
    "            'resource_id': '',\n",
    "            'news_code': code,\n",
    "            'url': url,\n",
    "            'title': title,\n",
    "            'datetime': d,\n",
    "            'sentences': sentences,\n",
    "            'result_code': 1\n",
    "        }\n",
    "\n",
    "        z = pickle.dumps(news)\n",
    "        id = hashlib.md5(z).hexdigest()\n",
    "        news['resource_id'] = id\n",
    "        return news\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'resource_id': '',\n",
    "            'news_code': code,\n",
    "            'url': url,\n",
    "            'title': '',\n",
    "            'datetime': '',\n",
    "            'sentences': [],\n",
    "            'result_code': 0\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day :  20190101\n",
      "Page No. 1 ( 2sec )\n",
      "Page No. 2 ( 5sec )\n",
      "Page No. 3 ( 2sec )\n",
      "Page No. 4 ( 4sec )\n",
      "Page No. 5 ( 2sec )\n",
      "Page No. 6 ( 3sec )\n",
      "Page No. 7 ( 2sec )\n",
      "Page No. 8 ( 3sec )\n",
      "Page No. 9 ( 4sec )\n",
      "Page No. 10 ( 2sec )\n"
     ]
    }
   ],
   "source": [
    "data = create_news_resource_links(20190101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'resource_id': '835ba30934ceb639da859cac22e94bf1',\n",
       " 'news_page_info': '20190101-1',\n",
       " 'counts': 15,\n",
       " 'news_ids': ['20190101235354271',\n",
       "  '20190101235344270',\n",
       "  '20190101235323266',\n",
       "  '20190101235315265',\n",
       "  '20190101234950238',\n",
       "  '20190101234936235',\n",
       "  '20190101234818223',\n",
       "  '20190101234600210',\n",
       "  '20190101234421201',\n",
       "  '20190101234342198',\n",
       "  '20190101234120183',\n",
       "  '20190101234111181',\n",
       "  '20190101234051177',\n",
       "  '20190101233342133',\n",
       "  '20190101232017067'],\n",
       " 'created_at': '2020-02-16 08:07:02',\n",
       " 'updated_at': '2020-02-16 08:07:02'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://news.daum.net/breakingnews/society/affair?page=1&regDate=20190101\n",
      "<li>\n",
      "<a class=\"link_thumb\" href=\"https://v.daum.net/v/20190101235354271\">\n",
      "<img alt=\"대피소에서 밤 보내는 주민들\" class=\"thumb_g\" src=\"https://img1.daumcdn.net/thumb/S95x77ht.u/?fname=https%3A%2F%2Ft1.daumcdn.net%2Fnews%2F201901%2F01%2FNEWS1%2F20190101235354821tudm.jpg&amp;scode=media\"/>\n",
      "</a>\n",
      "<div class=\"cont_thumb\">\n",
      "<strong class=\"tit_thumb\">\n",
      "<a class=\"link_txt\" href=\"https://v.daum.net/v/20190101235354271\">대피소에서 밤 보내는 주민들</a>\n",
      "<span class=\"info_news\">뉴스1<span class=\"txt_bar\"> · </span><span class=\"info_time\">23:53</span></span>\n",
      "</strong>\n",
      "<div class=\"desc_thumb\">\n",
      "<span class=\"link_txt\">\n",
      "                        (양양=뉴스1) 서근영 기자 = 1일 강원도 양양군 서면 송천리 일대 야산에서 발생한 산불로 대피한 정다운마을 주민들이 대피소인 상평초등학교에서 밤을 보내고 있다. 2...\n",
      "                    </span>\n",
      "</div>\n",
      "</div>\n",
      "</li>\n"
     ]
    }
   ],
   "source": [
    "url = make_news_page_url(20190101, 1)\n",
    "print(url)\n",
    "html = fetch_html(url)\n",
    "tags = select_tags(html, '#mArticle > div.box_etc > ul > li')\n",
    "print(tags[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "url = make_news_page_url(20190101, 3)\n",
    "print(url)\n",
    "html = request_url(url)\n",
    "# print(html)\n",
    "tags = select_tags(html, '#mArticle > div.box_etc > ul > li')\n",
    "# print(tags)\n",
    "print(len(tags))\n",
    "\n",
    "# tags[3].select('a')\n",
    "\n",
    "links = []\n",
    "for t in tags:\n",
    "    anchor = t.select_one('a')\n",
    "    print(anchor)\n",
    "    link = anchor['href'].replace('https://v.daum.net/v/', '')\n",
    "    links.append(link)\n",
    "links\n",
    "\n",
    "# print(len(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "html = request_url('https://news.v.daum.net/v/20190101235354271')\n",
    "data = create_news_data(html, '20190101235354271')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2019-01-01 ~ 2019-01-31 \n",
    "day = 20190101\n",
    "page_check = True\n",
    "page = 0\n",
    "logs = []\n",
    "while page_check == True:\n",
    "    page += 1\n",
    "    url = make_news_page_url(day, page)\n",
    "    page_list_html = request_url(url)\n",
    "    if page_list_html == None:\n",
    "        page_check = False\n",
    "        \n",
    "    list_tags = select_tags(page_list_html, '#mArticle > div.box_etc > ul > li')\n",
    "    if len(list_tags) == 0:\n",
    "        page_check = False\n",
    "        break\n",
    "    \n",
    "    # UTC\n",
    "    time_stamp = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time()))\n",
    "    log_info = {\n",
    "        'resource_id': None,\n",
    "        'news_page_info': str(day) + '-' + str(page),\n",
    "        'counts': len(list_tags),\n",
    "        'news_codes': None,\n",
    "        'created_at': time_stamp\n",
    "    }\n",
    "    code_list = []\n",
    "    for index, t in enumerate(list_tags):\n",
    "        anchor = t.select_one('a')\n",
    "        code = anchor['href'].replace('https://v.daum.net/v/', '')\n",
    "        code_list.append(code)\n",
    "        log_info['news_codes'] = code_list\n",
    "        \n",
    "    z = pickle.dumps(log_info)\n",
    "    id = hashlib.md5(z).hexdigest()\n",
    "    log_info['resource_id'] = id\n",
    "    logs.append(log_info)\n",
    "    \n",
    "    # Interval\n",
    "    n = randint(3, 10)\n",
    "    time.sleep(n)\n",
    "    print('Excute >>> ',n, 'sec')\n",
    "    print('Page number >>> ',page)\n",
    "\n",
    "print('Last Page Number: ', page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(logs)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = create_news_resource_links(20190101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(r)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start >  20190101\n",
      "day :  20190101\n",
      "Page No.1 ( 5sec )\n",
      "Page No.2 ( 2sec )\n",
      "Page No.3 ( 3sec )\n",
      "Page No.4 ( 5sec )\n",
      "Page No.5 ( 2sec )\n",
      "Page No.6 ( 3sec )\n",
      "Page No.7 ( 2sec )\n",
      "Page No.8 ( 4sec )\n",
      "Page No.9 ( 5sec )\n",
      "Page No.10 ( 2sec )\n",
      "Start >  20190102\n",
      "day :  20190102\n",
      "Page No.1 ( 5sec )\n",
      "Page No.2 ( 3sec )\n",
      "Page No.3 ( 2sec )\n",
      "Page No.4 ( 5sec )\n",
      "Page No.5 ( 3sec )\n",
      "Page No.6 ( 4sec )\n",
      "Page No.7 ( 2sec )\n",
      "Page No.8 ( 2sec )\n",
      "Page No.9 ( 4sec )\n",
      "Page No.10 ( 2sec )\n",
      "Page No.11 ( 3sec )\n",
      "Page No.12 ( 5sec )\n",
      "Page No.13 ( 3sec )\n",
      "Page No.14 ( 3sec )\n",
      "Page No.15 ( 4sec )\n",
      "Page No.16 ( 3sec )\n",
      "Page No.17 ( 3sec )\n",
      "Page No.18 ( 5sec )\n",
      "Page No.19 ( 5sec )\n",
      "Page No.20 ( 3sec )\n",
      "Page No.21 ( 5sec )\n",
      "Page No.22 ( 4sec )\n",
      "Page No.23 ( 5sec )\n",
      "Page No.24 ( 4sec )\n",
      "Start >  20190103\n",
      "day :  20190103\n",
      "Page No.1 ( 3sec )\n",
      "Page No.2 ( 2sec )\n",
      "Page No.3 ( 5sec )\n",
      "Page No.4 ( 4sec )\n",
      "Page No.5 ( 3sec )\n",
      "Page No.6 ( 3sec )\n",
      "Page No.7 ( 4sec )\n",
      "Page No.8 ( 5sec )\n",
      "Page No.9 ( 5sec )\n",
      "Page No.10 ( 5sec )\n",
      "Page No.11 ( 5sec )\n",
      "Page No.12 ( 2sec )\n",
      "Page No.13 ( 2sec )\n",
      "Page No.14 ( 5sec )\n",
      "Page No.15 ( 5sec )\n",
      "Page No.16 ( 5sec )\n",
      "Page No.17 ( 3sec )\n",
      "Page No.18 ( 4sec )\n",
      "Page No.19 ( 4sec )\n",
      "Page No.20 ( 3sec )\n",
      "Page No.21 ( 4sec )\n",
      "Page No.22 ( 2sec )\n",
      "Page No.23 ( 3sec )\n",
      "Start >  20190104\n",
      "day :  20190104\n",
      "Page No.1 ( 3sec )\n",
      "Page No.2 ( 5sec )\n",
      "Page No.3 ( 2sec )\n",
      "Page No.4 ( 3sec )\n",
      "Page No.5 ( 4sec )\n",
      "Page No.6 ( 3sec )\n",
      "Page No.7 ( 3sec )\n",
      "Page No.8 ( 4sec )\n",
      "Page No.9 ( 3sec )\n",
      "Page No.10 ( 3sec )\n",
      "Page No.11 ( 3sec )\n",
      "Page No.12 ( 3sec )\n",
      "Page No.13 ( 4sec )\n",
      "Page No.14 ( 5sec )\n",
      "Page No.15 ( 2sec )\n",
      "Start >  20190105\n",
      "day :  20190105\n",
      "Page No.1 ( 5sec )\n",
      "Page No.2 ( 4sec )\n",
      "Page No.3 ( 4sec )\n",
      "Page No.4 ( 4sec )\n",
      "Page No.5 ( 4sec )\n",
      "Page No.6 ( 3sec )\n",
      "Page No.7 ( 3sec )\n",
      "Start >  20190106\n",
      "day :  20190106\n",
      "Page No.1 ( 3sec )\n",
      "Page No.2 ( 2sec )\n",
      "Page No.3 ( 4sec )\n",
      "Page No.4 ( 3sec )\n",
      "Page No.5 ( 4sec )\n",
      "Page No.6 ( 5sec )\n",
      "Page No.7 ( 2sec )\n",
      "Page No.8 ( 2sec )\n",
      "Page No.9 ( 3sec )\n",
      "Start >  20190107\n",
      "day :  20190107\n",
      "Page No.1 ( 2sec )\n",
      "Page No.2 ( 3sec )\n",
      "Page No.3 ( 5sec )\n",
      "Page No.4 ( 3sec )\n",
      "Page No.5 ( 2sec )\n",
      "Page No.6 ( 3sec )\n",
      "Page No.7 ( 3sec )\n",
      "Page No.8 ( 3sec )\n",
      "Page No.9 ( 5sec )\n",
      "Page No.10 ( 4sec )\n",
      "Page No.11 ( 2sec )\n",
      "Page No.12 ( 3sec )\n",
      "Page No.13 ( 4sec )\n",
      "Start >  20190108\n",
      "day :  20190108\n",
      "Page No.1 ( 2sec )\n",
      "Page No.2 ( 3sec )\n",
      "Page No.3 ( 2sec )\n",
      "Page No.4 ( 4sec )\n",
      "Page No.5 ( 5sec )\n",
      "Page No.6 ( 4sec )\n",
      "Page No.7 ( 2sec )\n",
      "Page No.8 ( 5sec )\n",
      "Page No.9 ( 4sec )\n",
      "Page No.10 ( 3sec )\n",
      "Page No.11 ( 3sec )\n",
      "Page No.12 ( 4sec )\n",
      "Page No.13 ( 3sec )\n",
      "Page No.14 ( 5sec )\n",
      "Start >  20190109\n",
      "day :  20190109\n",
      "Page No.1 ( 4sec )\n",
      "Page No.2 ( 3sec )\n",
      "Page No.3 ( 2sec )\n",
      "Page No.4 ( 4sec )\n",
      "Page No.5 ( 5sec )\n",
      "Page No.6 ( 2sec )\n",
      "Page No.7 ( 5sec )\n",
      "Page No.8 ( 5sec )\n",
      "Page No.9 ( 5sec )\n",
      "Page No.10 ( 4sec )\n",
      "Page No.11 ( 3sec )\n",
      "Page No.12 ( 3sec )\n",
      "Page No.13 ( 4sec )\n",
      "Page No.14 ( 4sec )\n",
      "Page No.15 ( 5sec )\n",
      "Page No.16 ( 3sec )\n",
      "Page No.17 ( 4sec )\n",
      "Page No.18 ( 4sec )\n",
      "Page No.19 ( 4sec )\n",
      "Page No.20 ( 3sec )\n",
      "Page No.21 ( 2sec )\n",
      "Page No.22 ( 5sec )\n",
      "Page No.23 ( 3sec )\n",
      "Page No.24 ( 3sec )\n",
      "Page No.25 ( 3sec )\n",
      "Start >  20190110\n",
      "day :  20190110\n",
      "Page No.1 ( 4sec )\n",
      "Page No.2 ( 4sec )\n",
      "Page No.3 ( 4sec )\n",
      "Page No.4 ( 4sec )\n",
      "Page No.5 ( 2sec )\n",
      "Page No.6 ( 3sec )\n",
      "Page No.7 ( 5sec )\n",
      "Page No.8 ( 3sec )\n",
      "Page No.9 ( 5sec )\n",
      "Page No.10 ( 3sec )\n",
      "Page No.11 ( 4sec )\n",
      "Page No.12 ( 2sec )\n",
      "Page No.13 ( 5sec )\n",
      "Page No.14 ( 4sec )\n",
      "Page No.15 ( 4sec )\n",
      "Page No.16 ( 3sec )\n",
      "Page No.17 ( 3sec )\n",
      "Page No.18 ( 3sec )\n",
      "Start >  20190111\n",
      "day :  20190111\n",
      "Page No.1 ( 3sec )\n",
      "Page No.2 ( 2sec )\n",
      "Page No.3 ( 2sec )\n",
      "Page No.4 ( 5sec )\n",
      "Page No.5 ( 4sec )\n",
      "Page No.6 ( 3sec )\n",
      "Page No.7 ( 3sec )\n",
      "Page No.8 ( 2sec )\n",
      "Page No.9 ( 2sec )\n",
      "Page No.10 ( 4sec )\n",
      "Page No.11 ( 3sec )\n",
      "Page No.12 ( 5sec )\n",
      "Page No.13 ( 5sec )\n",
      "Page No.14 ( 2sec )\n",
      "Page No.15 ( 5sec )\n",
      "Page No.16 ( 3sec )\n",
      "Page No.17 ( 3sec )\n",
      "Page No.18 ( 3sec )\n",
      "Page No.19 ( 5sec )\n",
      "Page No.20 ( 2sec )\n",
      "Page No.21 ( 5sec )\n",
      "Page No.22 ( 4sec )\n",
      "Start >  20190112\n",
      "day :  20190112\n",
      "Page No.1 ( 4sec )\n",
      "Page No.2 ( 5sec )\n",
      "Page No.3 ( 2sec )\n",
      "Page No.4 ( 3sec )\n",
      "Page No.5 ( 2sec )\n",
      "Page No.6 ( 3sec )\n",
      "Page No.7 ( 2sec )\n",
      "Page No.8 ( 2sec )\n",
      "Page No.9 ( 4sec )\n",
      "Start >  20190113\n",
      "day :  20190113\n",
      "Page No.1 ( 3sec )\n",
      "Page No.2 ( 4sec )\n",
      "Page No.3 ( 2sec )\n",
      "Page No.4 ( 4sec )\n",
      "Page No.5 ( 5sec )\n",
      "Page No.6 ( 3sec )\n",
      "Page No.7 ( 5sec )\n",
      "Page No.8 ( 4sec )\n",
      "Page No.9 ( 2sec )\n",
      "Start >  20190114\n",
      "day :  20190114\n",
      "Page No.1 ( 2sec )\n",
      "Page No.2 ( 4sec )\n",
      "Page No.3 ( 4sec )\n",
      "Page No.4 ( 5sec )\n",
      "Page No.5 ( 5sec )\n",
      "Page No.6 ( 5sec )\n",
      "Page No.7 ( 3sec )\n",
      "Page No.8 ( 3sec )\n",
      "Page No.9 ( 4sec )\n",
      "Page No.10 ( 3sec )\n",
      "Page No.11 ( 5sec )\n",
      "Page No.12 ( 5sec )\n",
      "Page No.13 ( 4sec )\n",
      "Page No.14 ( 3sec )\n",
      "Page No.15 ( 4sec )\n",
      "Page No.16 ( 3sec )\n",
      "Page No.17 ( 3sec )\n",
      "Page No.18 ( 5sec )\n",
      "Page No.19 ( 4sec )\n",
      "Page No.20 ( 3sec )\n",
      "Start >  20190115\n",
      "day :  20190115\n",
      "Page No.1 ( 4sec )\n",
      "Page No.2 ( 2sec )\n",
      "Page No.3 ( 3sec )\n",
      "Page No.4 ( 5sec )\n",
      "Page No.5 ( 3sec )\n",
      "Page No.6 ( 3sec )\n",
      "Page No.7 ( 5sec )\n",
      "Page No.8 ( 2sec )\n",
      "Page No.9 ( 3sec )\n",
      "Page No.10 ( 4sec )\n",
      "Page No.11 ( 3sec )\n",
      "Page No.12 ( 5sec )\n",
      "Page No.13 ( 4sec )\n",
      "Page No.14 ( 3sec )\n",
      "Page No.15 ( 5sec )\n",
      "Page No.16 ( 3sec )\n",
      "Start >  20190116\n",
      "day :  20190116\n",
      "Page No.1 ( 2sec )\n",
      "Page No.2 ( 5sec )\n",
      "Page No.3 ( 2sec )\n",
      "Page No.4 ( 2sec )\n",
      "Page No.5 ( 5sec )\n",
      "Page No.6 ( 2sec )\n",
      "Page No.7 ( 3sec )\n",
      "Page No.8 ( 2sec )\n",
      "Page No.9 ( 5sec )\n",
      "Page No.10 ( 3sec )\n",
      "Page No.11 ( 5sec )\n",
      "Page No.12 ( 3sec )\n",
      "Page No.13 ( 3sec )\n",
      "Start >  20190117\n",
      "day :  20190117\n",
      "Page No.1 ( 4sec )\n",
      "Page No.2 ( 3sec )\n",
      "Page No.3 ( 5sec )\n",
      "Page No.4 ( 4sec )\n",
      "Start >  20190118\n",
      "day :  20190118\n",
      "Page No.1 ( 5sec )\n",
      "Page No.2 ( 3sec )\n",
      "Page No.3 ( 5sec )\n",
      "Page No.4 ( 5sec )\n",
      "Page No.5 ( 3sec )\n",
      "Page No.6 ( 4sec )\n",
      "Page No.7 ( 2sec )\n",
      "Page No.8 ( 3sec )\n",
      "Page No.9 ( 3sec )\n",
      "Page No.10 ( 4sec )\n",
      "Page No.11 ( 2sec )\n",
      "Page No.12 ( 3sec )\n",
      "Page No.13 ( 4sec )\n",
      "Page No.14 ( 2sec )\n",
      "Start >  20190119\n",
      "day :  20190119\n",
      "Page No.1 ( 5sec )\n",
      "Page No.2 ( 2sec )\n",
      "Page No.3 ( 4sec )\n",
      "Page No.4 ( 4sec )\n",
      "Page No.5 ( 4sec )\n",
      "Page No.6 ( 2sec )\n",
      "Start >  20190120\n",
      "day :  20190120\n",
      "Page No.1 ( 2sec )\n",
      "Page No.2 ( 5sec )\n",
      "Page No.3 ( 4sec )\n",
      "Page No.4 ( 4sec )\n",
      "Page No.5 ( 4sec )\n",
      "Page No.6 ( 5sec )\n",
      "Page No.7 ( 5sec )\n",
      "Start >  20190121\n",
      "day :  20190121\n",
      "Page No.1 ( 2sec )\n",
      "Page No.2 ( 5sec )\n",
      "Page No.3 ( 3sec )\n",
      "Page No.4 ( 5sec )\n",
      "Page No.5 ( 2sec )\n",
      "Page No.6 ( 4sec )\n",
      "Page No.7 ( 3sec )\n",
      "Page No.8 ( 5sec )\n",
      "Page No.9 ( 2sec )\n",
      "Page No.10 ( 3sec )\n",
      "Page No.11 ( 2sec )\n",
      "Page No.12 ( 3sec )\n",
      "Page No.13 ( 2sec )\n",
      "Page No.14 ( 5sec )\n",
      "Page No.15 ( 2sec )\n",
      "Page No.16 ( 4sec )\n",
      "Page No.17 ( 2sec )\n",
      "Page No.18 ( 2sec )\n",
      "Page No.19 ( 5sec )\n",
      "Start >  20190122\n",
      "day :  20190122\n",
      "Page No.1 ( 4sec )\n",
      "Page No.2 ( 2sec )\n",
      "Page No.3 ( 4sec )\n",
      "Page No.4 ( 3sec )\n",
      "Page No.5 ( 4sec )\n",
      "Page No.6 ( 4sec )\n",
      "Page No.7 ( 4sec )\n",
      "Page No.8 ( 2sec )\n",
      "Page No.9 ( 5sec )\n",
      "Page No.10 ( 5sec )\n",
      "Page No.11 ( 4sec )\n",
      "Page No.12 ( 3sec )\n",
      "Start >  20190123\n",
      "day :  20190123\n",
      "Page No.1 ( 4sec )\n",
      "Page No.2 ( 3sec )\n",
      "Page No.3 ( 2sec )\n",
      "Page No.4 ( 5sec )\n",
      "Page No.5 ( 3sec )\n",
      "Page No.6 ( 4sec )\n",
      "Page No.7 ( 3sec )\n",
      "Page No.8 ( 2sec )\n",
      "Page No.9 ( 3sec )\n",
      "Page No.10 ( 3sec )\n",
      "Page No.11 ( 2sec )\n",
      "Page No.12 ( 2sec )\n",
      "Page No.13 ( 3sec )\n",
      "Start >  20190124\n",
      "day :  20190124\n",
      "Page No.1 ( 4sec )\n",
      "Page No.2 ( 4sec )\n",
      "Page No.3 ( 4sec )\n",
      "Page No.4 ( 5sec )\n",
      "Page No.5 ( 5sec )\n",
      "Page No.6 ( 2sec )\n",
      "Page No.7 ( 2sec )\n",
      "Page No.8 ( 3sec )\n",
      "Page No.9 ( 4sec )\n",
      "Page No.10 ( 5sec )\n",
      "Page No.11 ( 2sec )\n",
      "Page No.12 ( 3sec )\n",
      "Page No.13 ( 3sec )\n",
      "Page No.14 ( 3sec )\n",
      "Page No.15 ( 3sec )\n",
      "Page No.16 ( 3sec )\n",
      "Page No.17 ( 3sec )\n",
      "Page No.18 ( 2sec )\n",
      "Page No.19 ( 2sec )\n",
      "Page No.20 ( 5sec )\n",
      "Page No.21 ( 4sec )\n",
      "Page No.22 ( 5sec )\n",
      "Page No.23 ( 3sec )\n",
      "Page No.24 ( 5sec )\n",
      "Page No.25 ( 5sec )\n",
      "Page No.26 ( 4sec )\n",
      "Page No.27 ( 4sec )\n",
      "Start >  20190125\n",
      "day :  20190125\n",
      "Page No.1 ( 3sec )\n",
      "Page No.2 ( 4sec )\n",
      "Page No.3 ( 4sec )\n",
      "Page No.4 ( 5sec )\n",
      "Page No.5 ( 4sec )\n",
      "Page No.6 ( 3sec )\n",
      "Page No.7 ( 4sec )\n",
      "Page No.8 ( 2sec )\n",
      "Page No.9 ( 4sec )\n",
      "Page No.10 ( 4sec )\n",
      "Page No.11 ( 5sec )\n",
      "Page No.12 ( 2sec )\n",
      "Page No.13 ( 2sec )\n",
      "Start >  20190126\n",
      "day :  20190126\n",
      "Page No.1 ( 2sec )\n",
      "Page No.2 ( 2sec )\n",
      "Page No.3 ( 4sec )\n",
      "Page No.4 ( 5sec )\n",
      "Page No.5 ( 5sec )\n",
      "Page No.6 ( 5sec )\n",
      "Page No.7 ( 4sec )\n",
      "Page No.8 ( 4sec )\n",
      "Start >  20190127\n",
      "day :  20190127\n",
      "Page No.1 ( 3sec )\n",
      "Page No.2 ( 5sec )\n",
      "Page No.3 ( 5sec )\n",
      "Page No.4 ( 5sec )\n",
      "Page No.5 ( 2sec )\n",
      "Page No.6 ( 4sec )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page No.7 ( 4sec )\n",
      "Start >  20190128\n",
      "day :  20190128\n",
      "Page No.1 ( 2sec )\n",
      "Page No.2 ( 3sec )\n",
      "Page No.3 ( 2sec )\n",
      "Page No.4 ( 5sec )\n",
      "Page No.5 ( 4sec )\n",
      "Page No.6 ( 2sec )\n",
      "Page No.7 ( 4sec )\n",
      "Page No.8 ( 4sec )\n",
      "Page No.9 ( 5sec )\n",
      "Page No.10 ( 4sec )\n",
      "Page No.11 ( 2sec )\n",
      "Page No.12 ( 5sec )\n",
      "Page No.13 ( 4sec )\n",
      "Page No.14 ( 5sec )\n",
      "Page No.15 ( 5sec )\n",
      "Page No.16 ( 2sec )\n",
      "Page No.17 ( 4sec )\n",
      "Page No.18 ( 2sec )\n",
      "Page No.19 ( 3sec )\n",
      "Page No.20 ( 3sec )\n",
      "Start >  20190129\n",
      "day :  20190129\n",
      "Start >  20190130\n",
      "day :  20190130\n",
      "Page No.1 ( 4sec )\n",
      "Page No.2 ( 5sec )\n",
      "Page No.3 ( 3sec )\n",
      "Page No.4 ( 4sec )\n",
      "Page No.5 ( 5sec )\n",
      "Page No.6 ( 5sec )\n",
      "Page No.7 ( 2sec )\n",
      "Page No.8 ( 2sec )\n",
      "Page No.9 ( 5sec )\n",
      "Page No.10 ( 2sec )\n",
      "Page No.11 ( 4sec )\n",
      "Page No.12 ( 5sec )\n",
      "Page No.13 ( 4sec )\n",
      "Page No.14 ( 3sec )\n",
      "Page No.15 ( 3sec )\n",
      "Page No.16 ( 2sec )\n",
      "Page No.17 ( 4sec )\n",
      "Start >  20190131\n",
      "day :  20190131\n",
      "Page No.1 ( 2sec )\n",
      "Page No.2 ( 2sec )\n",
      "Page No.3 ( 3sec )\n",
      "Page No.4 ( 2sec )\n",
      "Page No.5 ( 2sec )\n",
      "Page No.6 ( 2sec )\n",
      "Page No.7 ( 2sec )\n",
      "Page No.8 ( 3sec )\n",
      "Page No.9 ( 5sec )\n",
      "Page No.10 ( 2sec )\n",
      "Page No.11 ( 2sec )\n",
      "Page No.12 ( 4sec )\n",
      "Page No.13 ( 5sec )\n",
      "Page No.14 ( 3sec )\n",
      "Page No.15 ( 3sec )\n",
      "Page No.16 ( 4sec )\n",
      "Page No.17 ( 3sec )\n"
     ]
    }
   ],
   "source": [
    "# 2019-01-01 ~ 2019-01-31\n",
    "resources = []\n",
    "for i in range(1, 32):\n",
    "    if i < 10:\n",
    "        day = str(i).zfill(2)\n",
    "    else:\n",
    "        day = str(i)\n",
    "    d = '201901' + str(day)\n",
    "    print('Start > ', d)\n",
    "    resources += create_news_resource_links(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>resource_id</th>\n",
       "      <th>news_page_info</th>\n",
       "      <th>counts</th>\n",
       "      <th>news_ids</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0fcfc5a345062eada4da167f9a55ffa9</td>\n",
       "      <td>20190101-1</td>\n",
       "      <td>15</td>\n",
       "      <td>[20190101235354271, 20190101235344270, 2019010...</td>\n",
       "      <td>2020-02-16 08:11:55</td>\n",
       "      <td>2020-02-16 08:11:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38e57e5a978f78fa72ca83736d4eedec</td>\n",
       "      <td>20190101-2</td>\n",
       "      <td>15</td>\n",
       "      <td>[20190101231101022, 20190101225654935, 2019010...</td>\n",
       "      <td>2020-02-16 08:12:00</td>\n",
       "      <td>2020-02-16 08:12:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98a502d73139d1d73ed5f3fb1d02754f</td>\n",
       "      <td>20190101-3</td>\n",
       "      <td>15</td>\n",
       "      <td>[20190101214011317, 20190101213022233, 2019010...</td>\n",
       "      <td>2020-02-16 08:12:03</td>\n",
       "      <td>2020-02-16 08:12:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04dab3e7ffa4040611cddbbda6236552</td>\n",
       "      <td>20190101-4</td>\n",
       "      <td>15</td>\n",
       "      <td>[20190101192853305, 20190101192657292, 2019010...</td>\n",
       "      <td>2020-02-16 08:12:06</td>\n",
       "      <td>2020-02-16 08:12:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fb9744fdaaa3f2e43a76acb2d2533235</td>\n",
       "      <td>20190101-5</td>\n",
       "      <td>15</td>\n",
       "      <td>[20190101181803504, 20190101181802503, 2019010...</td>\n",
       "      <td>2020-02-16 08:12:11</td>\n",
       "      <td>2020-02-16 08:12:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2d103abab7294c59416bbc75532563ab</td>\n",
       "      <td>20190101-6</td>\n",
       "      <td>15</td>\n",
       "      <td>[20190101163802091, 20190101163011997, 2019010...</td>\n",
       "      <td>2020-02-16 08:12:14</td>\n",
       "      <td>2020-02-16 08:12:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>863e67031433329fb9c87a42600f8acc</td>\n",
       "      <td>20190101-7</td>\n",
       "      <td>15</td>\n",
       "      <td>[20190101140132858, 20190101135728812, 2019010...</td>\n",
       "      <td>2020-02-16 08:12:17</td>\n",
       "      <td>2020-02-16 08:12:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>d00343b9283b534c325690ff71c9457b</td>\n",
       "      <td>20190101-8</td>\n",
       "      <td>15</td>\n",
       "      <td>[20190101122339484, 20190101115633028, 2019010...</td>\n",
       "      <td>2020-02-16 08:12:19</td>\n",
       "      <td>2020-02-16 08:12:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ac91cb1911c58f6486523de9bd1c3b5b</td>\n",
       "      <td>20190101-9</td>\n",
       "      <td>15</td>\n",
       "      <td>[20190101094101302, 20190101093614215, 2019010...</td>\n",
       "      <td>2020-02-16 08:12:23</td>\n",
       "      <td>2020-02-16 08:12:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>63e64e748222c9c395c21aacdc909987</td>\n",
       "      <td>20190101-10</td>\n",
       "      <td>11</td>\n",
       "      <td>[20190101030610853, 20190101030021692, 2019010...</td>\n",
       "      <td>2020-02-16 08:12:29</td>\n",
       "      <td>2020-02-16 08:12:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>f7167dcc5aca12371c73c0c81b0653af</td>\n",
       "      <td>20190102-1</td>\n",
       "      <td>15</td>\n",
       "      <td>[20190102235912505, 20190102233528368, 2019010...</td>\n",
       "      <td>2020-02-16 08:12:32</td>\n",
       "      <td>2020-02-16 08:12:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>bc877f6b71935323e7e3136e21bdab13</td>\n",
       "      <td>20190102-2</td>\n",
       "      <td>15</td>\n",
       "      <td>[20190102221543878, 20190102221447868, 2019010...</td>\n",
       "      <td>2020-02-16 08:12:37</td>\n",
       "      <td>2020-02-16 08:12:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>22c77c31bb08c11d4acf0488e7971594</td>\n",
       "      <td>20190102-3</td>\n",
       "      <td>15</td>\n",
       "      <td>[20190102211638257, 20190102205400907, 2019010...</td>\n",
       "      <td>2020-02-16 08:12:40</td>\n",
       "      <td>2020-02-16 08:12:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>56a234e00aefe3e87161d532fd4ebd45</td>\n",
       "      <td>20190102-4</td>\n",
       "      <td>15</td>\n",
       "      <td>[20190102191951151, 20190102191800126, 2019010...</td>\n",
       "      <td>2020-02-16 08:12:43</td>\n",
       "      <td>2020-02-16 08:12:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>a6dd867205cd827d5b61fbfe741b8a67</td>\n",
       "      <td>20190102-5</td>\n",
       "      <td>15</td>\n",
       "      <td>[20190102174803134, 20190102174536055, 2019010...</td>\n",
       "      <td>2020-02-16 08:12:48</td>\n",
       "      <td>2020-02-16 08:12:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>d628a1cf6654b21928831e10fc8b9fc2</td>\n",
       "      <td>20190102-6</td>\n",
       "      <td>15</td>\n",
       "      <td>[20190102171100843, 20190102170726736, 2019010...</td>\n",
       "      <td>2020-02-16 08:12:51</td>\n",
       "      <td>2020-02-16 08:12:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6343a9b5abcf5578c89a9ef65db8c204</td>\n",
       "      <td>20190102-7</td>\n",
       "      <td>15</td>\n",
       "      <td>[20190102164341928, 20190102163952796, 2019010...</td>\n",
       "      <td>2020-02-16 08:12:56</td>\n",
       "      <td>2020-02-16 08:12:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>bb4a0ee90dc275654984c2ea281ce401</td>\n",
       "      <td>20190102-8</td>\n",
       "      <td>15</td>\n",
       "      <td>[20190102155917273, 20190102155732216, 2019010...</td>\n",
       "      <td>2020-02-16 08:12:58</td>\n",
       "      <td>2020-02-16 08:12:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>e0a0898739dc30e76255a03bebb54ec1</td>\n",
       "      <td>20190102-9</td>\n",
       "      <td>15</td>\n",
       "      <td>[20190102152153834, 20190102152100794, 2019010...</td>\n",
       "      <td>2020-02-16 08:13:00</td>\n",
       "      <td>2020-02-16 08:13:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>b25054dca6b53484455e610d455e67ba</td>\n",
       "      <td>20190102-10</td>\n",
       "      <td>15</td>\n",
       "      <td>[20190102144501316, 20190102144200178, 2019010...</td>\n",
       "      <td>2020-02-16 08:13:04</td>\n",
       "      <td>2020-02-16 08:13:04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         resource_id news_page_info  counts  \\\n",
       "0   0fcfc5a345062eada4da167f9a55ffa9     20190101-1      15   \n",
       "1   38e57e5a978f78fa72ca83736d4eedec     20190101-2      15   \n",
       "2   98a502d73139d1d73ed5f3fb1d02754f     20190101-3      15   \n",
       "3   04dab3e7ffa4040611cddbbda6236552     20190101-4      15   \n",
       "4   fb9744fdaaa3f2e43a76acb2d2533235     20190101-5      15   \n",
       "5   2d103abab7294c59416bbc75532563ab     20190101-6      15   \n",
       "6   863e67031433329fb9c87a42600f8acc     20190101-7      15   \n",
       "7   d00343b9283b534c325690ff71c9457b     20190101-8      15   \n",
       "8   ac91cb1911c58f6486523de9bd1c3b5b     20190101-9      15   \n",
       "9   63e64e748222c9c395c21aacdc909987    20190101-10      11   \n",
       "10  f7167dcc5aca12371c73c0c81b0653af     20190102-1      15   \n",
       "11  bc877f6b71935323e7e3136e21bdab13     20190102-2      15   \n",
       "12  22c77c31bb08c11d4acf0488e7971594     20190102-3      15   \n",
       "13  56a234e00aefe3e87161d532fd4ebd45     20190102-4      15   \n",
       "14  a6dd867205cd827d5b61fbfe741b8a67     20190102-5      15   \n",
       "15  d628a1cf6654b21928831e10fc8b9fc2     20190102-6      15   \n",
       "16  6343a9b5abcf5578c89a9ef65db8c204     20190102-7      15   \n",
       "17  bb4a0ee90dc275654984c2ea281ce401     20190102-8      15   \n",
       "18  e0a0898739dc30e76255a03bebb54ec1     20190102-9      15   \n",
       "19  b25054dca6b53484455e610d455e67ba    20190102-10      15   \n",
       "\n",
       "                                             news_ids           created_at  \\\n",
       "0   [20190101235354271, 20190101235344270, 2019010...  2020-02-16 08:11:55   \n",
       "1   [20190101231101022, 20190101225654935, 2019010...  2020-02-16 08:12:00   \n",
       "2   [20190101214011317, 20190101213022233, 2019010...  2020-02-16 08:12:03   \n",
       "3   [20190101192853305, 20190101192657292, 2019010...  2020-02-16 08:12:06   \n",
       "4   [20190101181803504, 20190101181802503, 2019010...  2020-02-16 08:12:11   \n",
       "5   [20190101163802091, 20190101163011997, 2019010...  2020-02-16 08:12:14   \n",
       "6   [20190101140132858, 20190101135728812, 2019010...  2020-02-16 08:12:17   \n",
       "7   [20190101122339484, 20190101115633028, 2019010...  2020-02-16 08:12:19   \n",
       "8   [20190101094101302, 20190101093614215, 2019010...  2020-02-16 08:12:23   \n",
       "9   [20190101030610853, 20190101030021692, 2019010...  2020-02-16 08:12:29   \n",
       "10  [20190102235912505, 20190102233528368, 2019010...  2020-02-16 08:12:32   \n",
       "11  [20190102221543878, 20190102221447868, 2019010...  2020-02-16 08:12:37   \n",
       "12  [20190102211638257, 20190102205400907, 2019010...  2020-02-16 08:12:40   \n",
       "13  [20190102191951151, 20190102191800126, 2019010...  2020-02-16 08:12:43   \n",
       "14  [20190102174803134, 20190102174536055, 2019010...  2020-02-16 08:12:48   \n",
       "15  [20190102171100843, 20190102170726736, 2019010...  2020-02-16 08:12:51   \n",
       "16  [20190102164341928, 20190102163952796, 2019010...  2020-02-16 08:12:56   \n",
       "17  [20190102155917273, 20190102155732216, 2019010...  2020-02-16 08:12:58   \n",
       "18  [20190102152153834, 20190102152100794, 2019010...  2020-02-16 08:13:00   \n",
       "19  [20190102144501316, 20190102144200178, 2019010...  2020-02-16 08:13:04   \n",
       "\n",
       "             updated_at  \n",
       "0   2020-02-16 08:11:55  \n",
       "1   2020-02-16 08:12:00  \n",
       "2   2020-02-16 08:12:03  \n",
       "3   2020-02-16 08:12:06  \n",
       "4   2020-02-16 08:12:11  \n",
       "5   2020-02-16 08:12:14  \n",
       "6   2020-02-16 08:12:17  \n",
       "7   2020-02-16 08:12:19  \n",
       "8   2020-02-16 08:12:23  \n",
       "9   2020-02-16 08:12:29  \n",
       "10  2020-02-16 08:12:32  \n",
       "11  2020-02-16 08:12:37  \n",
       "12  2020-02-16 08:12:40  \n",
       "13  2020-02-16 08:12:43  \n",
       "14  2020-02-16 08:12:48  \n",
       "15  2020-02-16 08:12:51  \n",
       "16  2020-02-16 08:12:56  \n",
       "17  2020-02-16 08:12:58  \n",
       "18  2020-02-16 08:13:00  \n",
       "19  2020-02-16 08:13:04  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(resources)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./data/source/news_texts_10.json', 'w', encoding='utf-8') as make_file:\n",
    "#      json.dump(news_resources, make_file, indent=\"\\t\")\n",
    "\n",
    "with open('./data/source/links_201901.json', 'w', encoding='utf-8') as make_file:\n",
    "    json.dump(resources, make_file, indent=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect a news data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "html_sample = request_url('https://news.v.daum.net/v/20190101212700201')\n",
    "sample = create_news_contents_data(html_sample, '20190101212700201')\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/source/links_201901.json', 'r') as f:\n",
    "    json_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = json.loads(json_data)\n",
    "links[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_page_info = links[0]['news_page_info'].split('-')\n",
    "day = news_page_info[0]\n",
    "page = news_page_info[1]\n",
    "news_codes = links[0]['news_codes']\n",
    "\n",
    "print(news_codes[0])\n",
    "print(day, page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "url = make_news_contents_url(news_codes[0])\n",
    "html = request_url(url)\n",
    "data = create_news_data(html, news_codes[0])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_resources = []\n",
    "num = 0\n",
    "for data in links[0:10]:\n",
    "    num += 1\n",
    "    print('\\n==========================')\n",
    "    print('Link order: ', num)\n",
    "    news_codes = data['news_codes']\n",
    "    t = []\n",
    "    for code in news_codes:\n",
    "        url = make_news_contents_url(code)\n",
    "        html = request_url(url)\n",
    "        resource = create_news_contents_data(html, code)\n",
    "        news_resources.append(resource)\n",
    "        # Interval\n",
    "        n = randint(1, 5)\n",
    "        time.sleep(n)\n",
    "        t.append(str(n) + 'sec')\n",
    "    print('Excute times: ', t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(news_resources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# news_resources[48]['sentences'] = \"NoneType' object has no attribute 'text'\"\n",
    "\n",
    "for (index, item) in emerate(news_resources):\n",
    "    if item['status']\n",
    "#     del news_resources['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/source/news_texts_10.json', 'w', encoding='utf-8') as make_file:\n",
    "     json.dump(news_resources, make_file, indent=\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
